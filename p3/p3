import numpy as np
import math

train_n = 100
test_n = 10000
d = 200

def calculateNormalizedError(a, X, y):
	numerator = np.linalg.norm(np.dot(X, a) - y)
	denominator = np.linalg.norm(y)

	return (numerator / float(denominator))


def calculateStochasticGradientDescent(step_size, lambdaVal):
	train_n = 100
	test_n = 10000
	d = 200
	X_train = np.random.normal(0,1, size=(train_n,d))
	a_true = np.random.normal(0,1, size=(d,1))
	y_train = X_train.dot(a_true) + np.random.normal(0,0.5,size=(train_n,1))
	X_test = np.random.normal(0,1, size=(test_n,d))
	y_test = X_test.dot(a_true) + np.random.normal(0,0.5,size=(test_n,1))
	
	#zero initialization
	a = np.zeros((d, 1))


	for iteration in range(0,50000):
		index = iteration % 100
		#gradient of the sum = sum of the gradients

		#sum of the gradients:
		#n x 1
		aX = np.dot(X_train[index], a) - (y_train[index])
		# aX = X.dot(a) - y

		#regularization factor
		regularizationFactor = lambdaVal * (np.linalg.norm(a))
		
		#n x d
		individualGradients = 2*np.dot(X_train[index].T.reshape(d,1), aX.reshape(1,1)) + 2*regularizationFactor
		#d x 1  
		# gradient = (np.sum(individualGradients, axis = 0)).reshape((d, 1))
		# print gradient

		a = a - step_size*(individualGradients)
		# print a

	# print calculateSquaredError(a, X, y)
	return calculateNormalizedError(a, X_test, y_test)


#used gradient descent instead of SGD
#used a regularization factor
#used non-zero intialization

#NEED TO BE CALCULTING **NORMALIZED** TEST ERROR, averaged over 1000 trials
def calculateTestErrorStochastic(step_size, lambdaVal):
	sumTestError = 0.0
	ran = 100
	for x in range(ran):
		sumTestError += calculateStochasticGradientDescent(step_size, lambdaVal)
	averageError = sumTestError/ float(ran)
	print averageError




lambdaVal = 0
stepSize = 0.0005
for i in range(30):
	# lambdaVal += i*0.01 
	calculateTestErrorStochastic(stepSize, lambdaVal + i*0.001)
	print 'finished gradient descent with lambda = ', lambdaVal + i*0.001 ,'and step size', stepSize, 'and 50000 iterations and 100 trials'

